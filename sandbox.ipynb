{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table dropped... \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "#Connecting to sqlite\n",
    "conn = sqlite3.connect('DB.sqlite')\n",
    "\n",
    "#Creating a cursor object using the cursor() method\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#Doping EMPLOYEE table if already exists\n",
    "cursor.execute(\"DROP TABLE inverted_index\")\n",
    "print(\"Table dropped... \")\n",
    "\n",
    "#Commit your changes in the database\n",
    "conn.commit()\n",
    "\n",
    "#Closing the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class Cleaning:\n",
    "    def __init__(self,doc):\n",
    "        self.contents=doc\n",
    "        self.content=''\n",
    "        for word in self.contents:\n",
    "            self.content += word\n",
    "        #print(self.content)\n",
    "    def Normalize(self):\n",
    "        return self.content.lower()\n",
    "\n",
    "    def Remove_uni(self):\n",
    "\n",
    "        self.content = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", self.Normalize())\n",
    "        return self.content \n",
    "\n",
    "    def Remove_stopw(self):\n",
    "        stop=stopwords.words('english')\n",
    "        remove_stop=self.content=\" \"\" \".join([word for word in word_tokenize(self.Remove_uni()) if word not in (stop)])\n",
    "        return remove_stop\n",
    "        \n",
    "    def Lemma(self):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        dum=[]\n",
    "        list_of_word=word_tokenize(self.Remove_stopw())\n",
    "        list_of_word.sort()\n",
    "        for word in list_of_word:\n",
    "            if word[-2:]=='ed' or word[-3:]=='ing':\n",
    "                dum.append(lemmatizer.lemmatize(word,'v'))\n",
    "            else:\n",
    "                dum.append(lemmatizer.lemmatize(word))\n",
    "        return dum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.conn = sqlite3.connect('DB2.sqlite')\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS inverted_index (\n",
    "                index_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                word TEXT NOT NULL,\n",
    "                frequency INTEGER NOT NULL,\n",
    "                websiteID INTEGER NOT NULL,\n",
    "                FOREIGN KEY(websiteID) REFERENCES websites(websiteID)\n",
    "            )\n",
    "        \"\"\")\n",
    "    def index_websites(self):\n",
    "        self.cursor.execute(\"SELECT websiteID, title, content FROM websites\")\n",
    "        websites = self.cursor.fetchall()\n",
    "        for website in websites:\n",
    "            website_id = website[0]\n",
    "            title = website[1]\n",
    "            content = website[2]\n",
    "            # Tokenize\n",
    "            words = []\n",
    "            if title:\n",
    "                title_words = Cleaning(title)\n",
    "                words += title_words.Lemma()\n",
    "            if content:\n",
    "                content_words = Cleaning(content)\n",
    "                words += content_words.Lemma()\n",
    "\n",
    "            word_freq = {}\n",
    "            for word in words:\n",
    "                if word in word_freq:\n",
    "                    word_freq[word] += 1\n",
    "                else:\n",
    "                    word_freq[word] = 1\n",
    "            for word, frequency in word_freq.items():\n",
    "                self.cursor.execute(\"INSERT INTO inverted_index (word, websiteID, frequency) VALUES (?, ?, ?)\", (word, website_id, frequency))\n",
    "        self.conn.commit()\n",
    "        print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "inv=InvertedIndex()\n",
    "inv.index_websites()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1000', '120', 'baht', 'book', 'book', 'hello', 'he', 'includes', 'look', 'love', 'love', 'name', 'preme', 'story']\n"
     ]
    }
   ],
   "source": [
    "a=Cleaning(\"Hello, his name is Preme. He's looks for a book that includes a love story. (love books) 1000 baht 120\")\n",
    "#a.Remove_uni()\n",
    "#a.Remove_stopw()\n",
    "print(a.Lemma())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1940afd46e860c7f332abaca86c7f263f79e738398e2627d7b4a413c6dbfc8bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
